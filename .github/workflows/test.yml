name: ML Pipeline CI

on:
  # push:
  #   branches: [ main, master  ]
  pull_request:
    #branches: [ main, master, ]
    branches: [ main, master, develop ]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pytest great_expectations pandas scikit-learn flake8 black mypy pytest-cov
        if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
        
    - name: Lint with flake8
      run: |
        flake8 day5/æ¼”ç¿’2 --count --select=E9,F63,F7,F82 --show-source --statistics
        flake8 day5/æ¼”ç¿’2 --count --exit-zero --max-complexity=10 --max-line-length=88 --statistics
        
    #- name: Format check with black
    #  run: |
    #    black --check day5/æ¼”ç¿’2
        
    #- name: Run data tests
    #  run: |
    #    pytest day5/æ¼”ç¿’2/tests/test_data.py -v
        
    #- name: Run model tests
    #  run: |
    #    pytest day5/æ¼”ç¿’2/tests/test_model.py -v

    - name: Run model validation
    id: validation
    run: |
      cd lecture-ai-engineering/day5/æ¼”ç¿’2
      python main.py --workflow
    continue-on-error: true
  
    - name: Upload metrics
      if: always()
      uses: actions/upload-artifact@v3
      with:
        name: model-metrics
        path: lecture-ai-engineering/day5/æ¼”ç¿’2/validation_results/latest_metrics.json
        if-no-files-found: error
    
    - name: Check model improvement
      if: steps.validation.outputs.has_previous == 'true'
      run: |
        if [ "${{ steps.validation.outputs.is_improved }}" = "true" ]; then
          echo "::notice::Model performance improved! ðŸŽ‰"
          echo "Accuracy: ${{ steps.validation.outputs.current_accuracy }} (Change: ${{ steps.validation.outputs.accuracy_change }})"
          echo "Inference Time: ${{ steps.validation.outputs.current_time }}s (Change: ${{ steps.validation.outputs.time_change }}s)"
        else
          echo "::warning::Model performance did not improve âš ï¸"
          echo "Accuracy: ${{ steps.validation.outputs.current_accuracy }} (Change: ${{ steps.validation.outputs.accuracy_change }})"
          echo "Inference Time: ${{ steps.validation.outputs.current_time }}s (Change: ${{ steps.validation.outputs.time_change }}s)"
        fi
    
    - name: Create comparison summary
      if: steps.validation.outputs.has_previous == 'true'
      run: |
        echo "## Model Performance Comparison" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### Accuracy" >> $GITHUB_STEP_SUMMARY
        echo "- Current: ${{ steps.validation.outputs.current_accuracy }}" >> $GITHUB_STEP_SUMMARY
        echo "- Previous: ${{ steps.validation.outputs.previous_accuracy }}" >> $GITHUB_STEP_SUMMARY
        echo "- Change: ${{ steps.validation.outputs.accuracy_change }}" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### Inference Time" >> $GITHUB_STEP_SUMMARY
        echo "- Current: ${{ steps.validation.outputs.current_time }}s" >> $GITHUB_STEP_SUMMARY
        echo "- Previous: ${{ steps.validation.outputs.previous_time }}s" >> $GITHUB_STEP_SUMMARY
        echo "- Change: ${{ steps.validation.outputs.time_change }}s" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### Status" >> $GITHUB_STEP_SUMMARY
        if [ "${{ steps.validation.outputs.is_improved }}" = "true" ]; then
          echo "âœ… Model performance improved!" >> $GITHUB_STEP_SUMMARY
        else
          echo "âš ï¸ Model performance did not improve" >> $GITHUB_STEP_SUMMARY
        fi  

    - name: Run pytest on main.py
      run: |
        pytest day5/æ¼”ç¿’2/main.py -v
    
    #ãƒ†ã‚¹ãƒˆé …ç›®è¿½åŠ 
    - name: Run pytest on model-validation.yml
      run: |
        pytest day5/æ¼”ç¿’2/.github/workflows/model-validation.yml -v
      
    


